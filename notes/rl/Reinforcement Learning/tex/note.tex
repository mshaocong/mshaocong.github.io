\newcommand{\calV}{\mathcal{V}}
\newcommand{\dpolicy}{\mathcal{D}^{\text{MD}}}
\newcommand{\rpolicy}{\mathcal{D}^{\text{MR}}}
\newcommand{\operator}{\mathscr{L}}

\textbf{Reinforcement Learning}:
\begin{displayquote}
``A RL agent interacts with an environment over time. At each time step $t$, the agent receives a state
$s_t$ in a state space $\calS$ and selects an action at from an action space $\calA$, following a policy $\pi(a_t|s_t)$,
which is the agent's behavior, i.e., a mapping from state $s_t$ to actions $a_t$, receives a scalar reward
$r_t$, and transitions to the next state $s_{t+1}$, according to the environment dynamics, or model, for
reward function $\calR(s,a)$ and state transition probability $\calP(s_{t+1}|s_t, a_t)$ respectively. In an episodic
problem, this process continues until the agent reaches a terminal state and then it restarts. The return
$R_t =\sum_{k=0}^\infty \gamma^k r_{t+k}$ is the discounted, accumulated reward with the discount factor $\gamma \in (0,1]$. The
agent aims to maximize the expectation of such long term return from each state \dots"
\end{displayquote} 
 
\section{Markov Decision Process (MDP)} 
We only consider the fully observable Markov decision process with finite discrete states, with finite discrete actions, and in discrete time steps.

\medspace

\textbf{Notations}:
\begin{itemize}
	\item State space $\calS$. Every element in $\calS$ is called a state. The random process $\{S_t\}$ represents the state at time $t$.
	
	\item Action space $\calA$. Every element in $\calA$ is called an action.  The random process $\{A_t\}$ represents the action taken by agent at time $t$.
	
	\item Reward $R: \calS\times \calA  \to \RR$. Reward is a scalar used to measure how well agent is doing by taking action $a$ in state $s$. The random process $\{R_t\}$ represents the reward received by agent at time $t$. Moreover, we assume for all $t$, $R_t \leq R$ for some $R<\infty$.
	
	\item History $H$. The history $H_t$ is defined as all information no later than time $t$.  
\end{itemize}

\medspace

\textbf{Agent}: The goal of agent is to gather rewards based on the received information from the environment; for example, we expect the behavior of an agent will maximize the expected, discounted, accumulative reward in the future. An RL agent may include one or more of these components: policy, value function, or model.
\begin{itemize}
	\item 
	A policy fully defines the agent's behavior.
	\begin{definition}[Policy]
		A \textit{deterministic policy} $\pi$ is a map from $\calS$ to $\calA$,
		$$\pi: s \mapsto \pi(s).$$ 
		A \textit{stochastic policy} $\pi$ is a distribution over actions $\calA$ given states,
		\begin{align*}
		\pi(a| s ) = \PP(A_t = a \mid S_t = s).
		\end{align*}
	\end{definition}  
	\begin{remark}
		We can also define a policy depending on the history, $\pi(a| H ) = \PP(A_t = a \mid H)$; it is same in the fully observable MDP case due to the Markov property of $\{S_t\}$.  
	\end{remark}
	\item The value function is a {prediction of future reward}; it is used to {evaluate the goodness/badness of states}.
	\begin{definition}[Value function]A \textit{return} from time-step $t$ with the discount $\gamma \in [0,1]$ is
		\begin{align*}
		G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}  + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}.
		\end{align*}
		A \textit{state-value function} $v_\pi: \calS \to \RR$ w.r.t. $\pi$ is defined as
		\begin{align*}
		v_\pi(s) := \EE_\pi[ G_t \mid S_t = s ] 
		\end{align*} 
		A \textit{action-value function} $q_\pi: \calS \times \calA \to \RR$ w.r.t. $\pi$  is defined as 
		\begin{align*}
		q_\pi(s,a) := \EE_\pi[ G_t \mid S_t = s, A_t = a ]
		\end{align*}  
		The \textit{optimal state-value function} $v_\ast:\calS \to \RR$ is defined as 
		$$v_\ast(s) := \max_\pi v_\pi(s).$$
		The \textit{optimal action-value function} $q_\ast: \cal \calS \times \calA \to \RR$ is defined as 
		$$q_\ast(s,a) := \max_\pi q_\pi(s,a).$$
	\end{definition}
\begin{remark} 
Note that the value function can also be considered as a function of policy $\pi$. Therefore, we can define the optimal policy as the policy maximizing the value function.
\end{remark} 
\begin{comment}
\begin{remark} Let $V$ be the space of all value functions (with the same domain); here we take state-value functions as an example. There is a natural partial order on $V$:
\begin{align*}
v(s) \leq v'(s) \iff v(s) \leq v'(s) \text{ for all } s\in \calS.
\end{align*}
\begin{enumerate}[(1)]
\item 
Sometimes we use $\sup_\pi$ instead of $\max_\pi$ in the definition of optimal value function; because  

\item 
Note that the value function can also be considered as a function of policy $\pi$. Therefore, we can define the optimal policy as the policy maximizing the value function.
\end{enumerate}
\end{remark}
\end{comment}
	\begin{definition}[Optimal Policy]
		A policy $\pi_\ast$ is optimal in $\mathcal{D}$, if for any policy $\pi\in \mathcal{D}$ and for all $s\in \calS$,
		$$v_{\pi_\ast}(s) \geq v_\pi(s).$$ 
	\end{definition} 
	
	\item A model predicts what the environment will do next.
	\begin{definition}[Model]
		$\calP$ is defined as the distribution of next step,
		\begin{align*}
		\calP^a_{ss'} = \PP\left[ S_{t+1} = s' \mid S_t = s, A_t = a \right].
		\end{align*}
		$\calR$ is defined as the next expected reward,
		\begin{align*}
		\calR_s^a = \EE [ R_{t+1} \mid S_t = s, A_t = a  ].
		\end{align*}
	\end{definition}
\end{itemize}  

\medspace

\textbf{Probability Review}: A random process $\{S_t\}_{t\in \NN}$ with a finite state space $\calS$ is called a Markov process if for every $s, s_1, \dots, s_t\in \calS$,
\begin{align*}
 \PP( S_{t+1} = s \mid S_1 = s_1, \dots, S_t = s_t ) = \PP( S_{t+1} = s \mid S_t = s_t );
\end{align*} 
or equivalently, it could be defined relative to a filtration $\calF$,
\begin{align*}
\EE( f(S_{t+1}) \mid \calF_t ) = \EE( f(S_{t+1}) \mid S_t ) 
\end{align*}
for any measurable function $f:\calS \to \RR$. 

Given a Markov process $\{S_t\}_{t\in \NN}$, the state transition probability from $s$ to $s'$ is written as
\begin{align*}
\calP_{ss'} = \PP\left( S_{t+1} = s' \mid S_t = s \right);
\end{align*}
It forms the state transition matrix
\begin{align*}
\calP = \begin{pmatrix}
\calP_{11} & \dots & \calP_{1n} \\
\vdots & & \\
\calP_{n1} & \dots & \calP_{nn}
\end{pmatrix}.
\end{align*} 

\subsection*{Markov Decision Process} 
There are two similar concepts: Markov reward process (MRP) and Markov decision process (MDP). In the MRP model, the history is considered as the filtration generated by $\{S_t\}$, while in the MDP model, the history is considered as the filtration generated by $\{S_t\}$ and $\{A_t\}$.  
\begin{definition}
	\label{MDP}
	A \textit{Markov Reward process} is a four tuple $\langle \calS, \calP, \calR, \gamma \rangle$:
	\begin{itemize}
		\item A finite state space $\calS$. 
		
		\item A transition matrix $\calP$; that is,
		\begin{align*}
		\calP_{ss'} = \PP\left( S_{t+1} = s' \mid S_t = s \right).
		\end{align*} 
		
		\item A reward function $\calR:\calS \to \RR$ defined as
		\begin{align*}
		\calR: s \mapsto \EE[ R_{t+1} \mid S_t = s  ].
		\end{align*}
		where $R_{t+1}$ is the reward at time $t+1$. 
		\item A discount factor $\gamma \in [0,1]$.
	\end{itemize} 
A \textit{Markov decision process} is a five tuple $\langle \calS, \calA, \calP, \calR, \gamma \rangle$:
	\begin{itemize}
		\item A finite state space $\calS$. 
		
		\item A finite action space $\calA$.
		
		\item A transition matrix $\calP$; that is,
		\begin{align*}
		\calP^a_{ss'} = \PP\left( S_{t+1} = s' \mid S_t = s, A_t = a \right).
		\end{align*} 
		
		\item A reward function $\calR:\calS \times \calA \to \RR$ defined as
		\begin{align*}
		\calR: (s,a) \mapsto \EE[ R_{t+1} \mid S_t = s , A_t = a ].
		\end{align*}
		where $R_{t+1}$ is the reward at time $t+1$. 
		\item A discount factor $\gamma \in [0,1]$.
	\end{itemize}
\end{definition}  
 

\begin{example}[Policies in MDP]
	Given a MDP $\langle \calS, \calA, \calP, \calR, \gamma \rangle$ with policy $\pi$, the agent's action will be leaded as follow:
	\begin{enumerate}[1)] 
		\item Start from time $t$ with an initial state $S_t = s$. 
		\item Take an action based on the policy: $A_t \sim \pi(  \cdot  |S_t = s )$.
		\item Compute the reward: $(s, a) \mapsto \calR(s, a) $.
		\item Move to the next state based on the transition kernel: $S_{t+1} \sim \PP( \cdot \mid S_t =s, A_t = a )$. 
	\end{enumerate} 
\end{example}


\begin{theorem}[Bellman Equation]\label{bellman_equation}
	The state-value function can be decomposed into the sum of immediate
	reward and discounted value of successor state,
	\begin{align*}
		v_\pi(s) = \EE_\pi [ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s ].
	\end{align*}
	The action-value function can similarly be decomposed,
	\begin{align*}
		q_\pi = \EE_\pi [R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a].
	\end{align*}
\end{theorem}
\begin{proof}
	Directly by the definition of $v_\pi(s)$:
	\begin{align*}
		v_\pi(s) &= \EE_\pi  [  R_{t+1} + \gamma \sum_{k=1}^\infty \gamma^{k-1} R_{t+k+1} \mid  S_t = s  ] \\
		&= \EE_\pi\left[  R_{t+1} + \gamma  \EE_\pi[\sum_{k=1}^\infty \gamma^{k-1} R_{t+k+1} \mid S_{t+1} ]  \Bigg| S_t = s \right]\\
		&= \EE_\pi [  R_{t+1} + \gamma  v_\pi(S_{t+1})  \mid S_t = s  ]
	\end{align*}
	
	The action-value case is omitted.
\end{proof}
\begin{remark}There are two other equivalent representations of Bellman expectation equation,
	\begin{enumerate}[1)]
		\item 
		The Bellman expectation equation can be written as the matrix form as
		\begin{align*}
		q_\pi = \calR^\pi + \gamma \calP^\pi q_\pi
		\end{align*}
		with direct solution
		\begin{align*}
		q_\pi = (I - \gamma \calP^\pi)^{-1} \calR^\pi.
		\end{align*}  
		
		\item 
		Or we can write it more explicitly,
		\begin{align*}
		  q_\pi(s,a) =  \calR^\pi (s, a  ) + \gamma \sum_{s'\in\calS} \calP^{a  }_{ss'} q_\pi(s',a).
		\end{align*}
		
	\end{enumerate}
	
\end{remark}


\subsection*{Optimal Policy} 
In this part, we will show the existence of optimal value function. Let $\dpolicy$  be the space of all deterministic policies and $\rpolicy$ be the space of all stochastic polices (of course, $\dpolicy \subset \rpolicy$).  First, we begin from the Bellman optimality equation. 
\begin{theorem}[Bellman Optimality Equation]
	\label{bellmam_optimality} Let $v_\ast, q_\ast$ be the optimal state-value function and the optimal action-value function, then
	\begin{align*}
		v_\ast(s) &= \max_a q_\ast(s,a)\\
		q_\ast(s,a) &= \calR_s^a + \gamma \sum_{s'\in\calS} \calP_{ss'}^a v_\ast(s').
	\end{align*}
\end{theorem}
\begin{proof}
	First, we notice that  
	\begin{align*}
		v_\pi(s) =   \sum_{a\in \calA} q_\pi(s,a) \pi(a|s) \label{def_of_v} .
	\end{align*}
	If $q_\ast$ is the optimal value function, then 
	\begin{align*}
	\pi_\ast(a|s) = \begin{cases}
	1 & \text{if }s= \argmax_{a \in \calA} q_\ast (s,a)\\
	0 & o.w.
	\end{cases} 
	\end{align*}
	is the optimal policy (it could be shown by the definition of optimal policy). Then
	\begin{align*}
		\max_\pi v_\pi(s) = \max_{a \in \calA} q_\ast(s,a).
	\end{align*}
	Plug the optimal policy into the Bellman expectation equation, then we can get the second equation.
\end{proof}
\begin{remark} We can also write it as below:
	\begin{align*}
	v_\ast(s) &= \max_a \Big[\calR^a_s + \gamma \sum_{s'\in\calS} \calP_{ss'}^a v_\ast(s')\Big]; \\
	q_\ast(s,a) &= \calR_s^a + \gamma \sum_{s'\in\calS} \calP_{ss'}^a  \max_{a'} q_\ast(s', a').
	\end{align*}
	This form gives us an anther perspective of the optimal value function; if we define an operator 
	\begin{align*}
		L: v \mapsto \max_a \Big[ \calR_s^a + \gamma \sum_{s'\in\calS} \calP_{ss'}^a v(s') \Big],
	\end{align*}
	then $v_\ast$ is a stationary point of this operator (that is, $Lv_\ast = v_\ast$). The existence of optimal value function will be immediately implied by the contraction of $L$.
\end{remark}

\medspace

Let $\calV$ be the space of all bounded functionals on $\calS$; it is a Banach space with the supremum norm 
$$\| v \| := \max_{s \in \calS } v(s).$$
The Bellman optimality equation defines an operator $L: \calV \to \calV$ as 
\begin{align*}
L:\ v  \mapsto \max_{\pi \in \dpolicy} \{  \calR^\pi  + \gamma \calP^\pi v \}. 
\end{align*}
More explicitly,
\begin{align*}
Lv(s) &= \max_{\pi \in \dpolicy} \{  \calR^\pi  + \gamma \calP^\pi v  \}(s) \\
&= \max_{a \in \calA}  \{  \calR^a_s + \gamma \sum_{s'\in\calS} \calP^a_{ss'} v(s') \},
\end{align*}
where the maximum is attained at 
\begin{align*}
a_\ast^s := \argmax_{a \in \calA}  \{  \calR^a_s + \gamma \sum_{s'\in\calS} \calP^a_{ss'} v(s') \}.
\end{align*}

\begin{lemma}
\label{contraction}
Suppose that the discount $\gamma\in (0,1)$. Then $L:\calV \to \calV$ is a contraction operator.
\end{lemma}
\begin{proof}
Let $u,v \in \calV$. Without loss of generality, fix $s\in \calS$ such that $Lv(s) \geq Lu(s)$. Then
\begin{align*}
Lv(s) - Lu(s) &= \left[\calR(s, a^s_\ast ) + \gamma \sum_{s'\in\calS} \calP^{a^s_\ast }_{ss'} v(s')\right] - \left[\calR(s, a^s_\ast ) + \gamma \sum_{s'\in\calS} \calP^{a^s_\ast }_{ss'} u(s')\right] \\
&= \gamma \sum_{s'\in\calS} \calP^{a^s_\ast }_{ss'} [v(s') - u(s')] \\
&\leq \gamma \| v - u\|.
\end{align*}
Therefore, for all $s \in \calS$,
$$| Lv(s) - Lu(s) | \leq \gamma \| v - u \|;$$
it implies $\| Lv - Lu\| \leq \gamma \| v- u\|$.
\end{proof}
\begin{remark}
Under additional technical conditions, this result also holds for more general state spaces.
\end{remark}
  
\medspace

Now we can prove the fundamental result related to MDP.
\begin{theorem}
	There is always a deterministic optimal policy for the MDP defined in Definition \ref{MDP} with discount $\gamma \in (0,1)$.  
\end{theorem}
\begin{proof}  
	The proof would be divided into three part:
	\begin{itemize}
		\item Existence and uniqueness of $v_\ast$: By Lemma \ref{contraction} and Banach fixed-point theorem, there exists unique $v_\ast \in \calV$ such that 
		$$L v_\ast = v_\ast;$$
		moreover, for any $v\in \calV$, the sequence $v_n:= L^n v$ converges to $v_\ast$ in norm.
		
		\item Construction of $q_\ast$: By Theorem \ref{bellmam_optimality}, the optimal action-value function is
		$$q_\ast(s,a) = \calR_s^a + \gamma \sum_{s'\in\calS} \calP_{ss'}^a v_\ast(s').$$
		
		\item Construction of $\pi_\ast$: Define 
		\begin{align*}
			\pi_\ast: s \mapsto \argmax_{a \in \calA} q_\ast (s,a);
		\end{align*}
		it is the deterministic optimal policy for the given MDP.
	\end{itemize}
	
\end{proof}

\begin{comment}
\begin{theorem} 
For any Markov decision process as in Definition \ref{MDP} with discount $\gamma \in (0,1)$, 
\begin{enumerate}[a)]
\item  there exists a unique $v_\ast \in V$ such that $Lv_\ast  = v_\ast$;

\item  $\pi_\ast \in \rpolicy$ is optimal if and only if $v_{\pi^\ast} = L v_{\pi^\ast}$. 
\end{enumerate}

\end{theorem} 
\end{comment}
\begin{comment}
\begin{example}[Tic-Tac-Toe]
A state of the game tic-tac-toe: 
\begin{center}
\begin{tabular}{c | c | c}
x & o & o \\
\hline
o & x & x \\
\hline
&   &  x \\
\end{tabular}
\end{center}

\end{example}

\medspace 

\noindent
\textbf{Assumptions:} 
\begin{enumerate}[start=1,label={(\bfseries H\arabic*)}]
\item \label{ass1} \textbf{Reward Hypothesis}:

All goals can be described by the maximisation of expected
cumulative reward.

\item \label{ass2} \textbf{Information state/Markov state}:

The state process forms a Markov chain; that is,
\begin{align*}
\PP( S_{t+1} \mid S_t ) = \PP( S_{t+1} \mid S_1, \dots, S_t ).
\end{align*}
It implies
\begin{itemize}
\item The state captures all relevant information from the history.
\item Once the state is known, the history may be thrown away; i.e. The state is a sufficient statistic of the future.
\end{itemize}

\item \label{ass3} \textbf{Full observability}. 
\end{enumerate}

\end{comment}


 


