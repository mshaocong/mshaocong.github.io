\section{Maximum likelihood}
 
\subsection{Maximum likelihood estimators (MLE)}
\begin{mydef}
	Let $\FF_\Theta$ be a family of pmfs/pdfs. 
	\begin{itemize}
		\item 
		\uline{The likelihood function} is
		$$L(\theta; x) = f_\theta (x),\quad \theta \in \Theta.$$
		
		\item
		\uline{The log-likelihood} is 
		$$l(\theta;x) = \log L(\theta; x).$$
	\end{itemize}
\end{mydef}
\begin{remark}
	If $X_i \SimIID f_\theta$, then $L(\theta;X) = \prod_{i=1}^n f_\theta(X_i)$ and $l(\theta; X) =\sum_{i=1}^n \log f_\theta(X_i).$
\end{remark}
{\color{blue}
\begin{mydef}
	If $X_i \SimIID f_\theta$ and $X = x$ is observed. 
	$$\hat{\theta}(x) = {\arg \max} _{\theta \in \Theta} L(\theta; x),$$
	if it exists, is called a \uline{maximum likelihood estimate of $\theta$}. 
\end{mydef}
}\begin{remark}
	By the strict monotonity of $\log$, we have
	$$\hat{\theta}(x) = {\arg \max} _{\theta \in \Theta} l(\theta; x) = {\arg \max} _{\theta \in \Theta} \sum_{i=1}^n f_\theta (x_i).$$
\end{remark}

\begin{exap}
	$X_i \SimIID \mathrm{Poisson}(\theta),\ \Theta = (0, \infty)$. 
	
	Compute its likelihood function:
	\begin{align*}
		L(\theta; x) &= e^{-n\theta} \cdot \frac{e^{(\log \theta)\cdot \sum x_i}}{\prod x_i!}\\
		l(\theta; x) &= (\sum x_i) \log \theta - n \theta - \sum \log(x_i !)
	\end{align*}
	
	Compute its partial derivatives:
	\begin{align*}
		\frac{\partial}{\partial \theta} &=  \frac{\sum x_i}{\theta} - n = 0 \implies \ \theta = \bar{x} \\
		\frac{\partial^2}{\partial \theta^2} &= - \frac{\sum x_i}{\theta^2} \leq 0
	\end{align*}
	
	Thus, $\hat{\theta}(x) = \bar{x}$ is the MLE except when $\bar{x} = 0$; because when $\bar{x} = 0$, $\theta = 0 \notin \Theta$.
\end{exap}

\begin{exap}
	$X_i \SimIID U(\theta_1, \theta_2)$.
	
	Compute its likelihood function:
	\begin{align*}
		L(\theta; x) & = \prod f_i(x_i) = \prod\Big(  \frac{1}{\theta_2 - \theta_1} \IndOne(\theta_1 \leq x_i \leq \theta_2) \Big)  \\
		&= \begin{cases}
		0 &\quad \theta_1 \geq x_{(1)} \text{ or } \theta_2 < x_{(n)} \\
		\frac{1}{(\theta_2 - \theta_1)^n} &\quad \text{o.w.}
		\end{cases}
	\end{align*}
	
	Notice: when $ \theta_1 \leq x_{(1)}$ and $\theta_2 \geq x_{(n)}$,
	$$(\theta_2 - \theta_1) \downarrow \implies \ L(\theta; x) \uparrow.$$
	
	Therefore, $(\hat{\theta}_1, \hat{\theta}_2) = (x_{(1)}, x_{(n)})$ is the MLE.
\end{exap}
{
\begin{prop}
	Let $T$ be sufficient for $\theta$ for a family of pdfs/pmfs. If an MLE exists, there is an MLE such that $\hat{\theta} = g(T)$.
\end{prop}
}\begin{proof}
	Compute its likelihood function:
	\begin{align*}
		L(\theta; x) & = f_\theta (x)   \\
		(\text{By Thm 1.4.}) \quad &= h(x) g_\theta (T(x))  
	\end{align*}
	
	Assume $\theta^\ast$ maximizes $L(\theta;x)$. It also maximizes $w_x(\theta) = g_\theta (T(x))$.
	
	Define $S(x) = \{ \theta^\ast \in \Theta: g_{\theta^\ast} (T(x)) = \max_{\theta} g_\theta (T(x)) \}$. (Note: the maxima may not be unique.)
	
	%为什么?
	Notice that $T(x) = T(y) \implies S(x) = S(y)$, so we can choose $\hat{\theta}(x) \in S(x)$ such that it is a function of $T(x)$. 
\end{proof}
 
\subsection{Uniqueness and existence of MLEs}

The following example shows: (1) MLE may not be unique. (2) MLE could be a function of $T$; however, some MLEs may not be a function of $T$.
\begin{exap}
	$X_i \SimIID U(\theta-1, \theta+1)$.
	
	Compute its likelihood function:
	\begin{align*}
		L(\theta; x) &= \frac{1}{2^n} \cdot \IndOne(x_{(1)} \geq \theta -1 ) \cdot \IndOne (x_{(n)} \leq \theta+1)\\
		&= \frac{1}{2^n} \cdot \IndOne(x_{(n)} - 1 \leq \theta \leq  x_{(1)} + 1)
	\end{align*}
	 $\implies$ any estimator $\hat{\theta}(x) \in [x_{(n)} - 1, x_{(1)} + 1]$ is an MLE. (not unique)
	
	In particular, 
	$$\hat{\theta}(x) = \alpha (x_{(n)} - 1) + (1 -\alpha) (x_{(1)} + 1)$$
	for $0\leq \alpha \leq 1$ is an MLE that is a function of $T = (x_{(1)}, x_{(n)})$; however, so is 
	$$\sin^2(\bar{x}) (x_{(n)} - 1)+ \cos^2(\bar{x}) (x_{(1)} + 1) ,$$
	not a function of $T$.
\end{exap}
\begin{thm}
	\textbf{ }
	
	\begin{itemize}
		\item \textbf{Existence}
		
		Suppose $l: \Theta \to \RR$, $\Theta$ open in $\RR^k$, is continuous. If  $l(\theta;x) \to -\infty$ as $\theta \to \partial \Theta$, then 
		$$\{ \theta \in \Theta: l(\theta) = \max_{\theta \in \Theta} l(\theta)  \} \neq \emptyset.$$
		
		\item \textbf{Existence and uniqueness}
		
		Suppose $X \sim f_\theta$, $\theta \in \Theta \subset \RR^k$ open set. If $l(\theta;x)$ is strictly convave, is continuous, and moreover, $l(\theta;x) \longrightarrow -\infty$~as~$\theta \to \partial \Theta$, then the MLE exists and is unique. 
	\end{itemize}
	
\end{thm} 

\subsection{Exponential family: Part II}
\begin{lem}
	Let $\FF_\eta$ be a k-parameter exponential family in canonical parameter. The following statements are equivalent:
	\begin{enumerate}[a)]
		\item The log-likelihood function $l(\eta; x)$ is strictly convave 
		\item $A(\eta)$ is strictly convex
		\item $A''(\eta) = \Var(T) > 0$ (aka full rank).
	\end{enumerate}
\end{lem}

\begin{thm}
	Suppose $\FF_\Theta$ is a k-parameter exponential family with 
	$$f_\eta = h(x) \exp \Big \{   \sum_{j=1}^k \eta_j T_j(x) - A(\eta) \Big \}$$
	such that $\Theta$ is open and $A''(\eta) > 0$. Let $x$ be the deserved value and $t_0 = T(x) \in \RR^k$.
	\begin{enumerate}[a)]
		\item If $\PP_\eta (c^T T(x) > c^T t_0) > 0$ for all $c \neq 0$, $\eta \in \Theta$, then $\hat{\eta}$ exists, is unique, and satisfies
		$$A' \big(\hat{\eta} (x) \big) = \EE_{\hat{\eta}(x)} ( T(x) ) = t_0.$$ 
		\item If $\exists c\neq 0$ such that $\PP(c^T T(x) > c^T t_0) = 0$, there is no MLE. 
	\end{enumerate}
\end{thm}

\begin{cor}
	Let $C_T$ be the convex hull of the support of $T$. Then the MLE exists and is unique if and only if $t_0 \in C_T^\circ$.
\end{cor}

\begin{cor}
	If $T$ has a continuous distribution, the MLE exists and is unique.
\end{cor}

\begin{cor}
	Let the exponential family be 
	$$f_\theta(x) = h(x) \exp \Big\{  \sum_{j=1}^k Q_j(\theta) T_j(x) - B(\theta)  \Big\}.$$
	If $\EE_\theta T_j = T_j$ have a solution $\hat{\theta}(X) \in Q(\Theta)^\circ$, it is the unique MLE.
\end{cor}

\begin{exap}
	$X\sim \mathrm{Bin}(n, \theta)$. Then $\hat{\theta} = \frac{X}{n}$ is the MLE unless $X=0$.
\end{exap}

\begin{exap}
	$X_i \SimIID \mathrm{Gamma}(\alpha, \beta)$. The MLE exists and is unique.
\end{exap}

\subsection{Invariance}
{\color{blue}
\begin{thm}
	Let $\FF_\theta$ be a family of pdfs/pmfs, $\theta \in \RR^k$. 
	If $\hat{\theta}$ is an MLE and $h:\RR^k \to \RR^p$ with $p\leq k$, then $h(\hat{\theta})$ is an MLE for $h(\theta)$.
\end{thm}} 

\begin{exap}
	$X_i \SimIID N(\mu, \sigma^2),\ \mu \in \RR$ and $\sigma>0$. Obviously, $\hat{\mu} = \bar{X}$ and $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$ are MLEs for $\mu$ and $\sigma^2$. We may be interested in the MLE of $\mu/ \sigma$.
	
	Let $h: (x,y) \mapsto \frac{x}{y}$, then $h(\hat{\mu}, \hat{\sigma})$ is the MLE for $h(\mu, \sigma)$. Thus, the MLE for $\mu/ \sigma$ is $\bar{X}/\hat{\sigma}$.
	
\end{exap}

\subsection{Asymptotic consistency and normality}
\begin{thm}[Wald]
	Recall that $D(\theta_0, \theta) = \EE_{\theta_0} ( \log f_\theta(x) )$. Suppose 
	$$ \sup_{\theta \in \Theta} \Big(  \frac{1}{n}\sum_{i=1}^n \log f_\theta (x) - D(\theta_0, \theta)  \Big) \xrightarrow[\theta_0]{\PP} 0,$$
	and for all $\epsilon>0$, 
	$$\sup_{ \theta:| \theta - \theta_0 |\geq \epsilon } D(\theta_0, \theta) < D(\theta_0, \theta_0).$$
	
	Then we have
	$$\hat{\theta} \xrightarrow[\theta_0]{\PP} \theta_0.$$
\end{thm}
\begin{remark}
	Generally, consistency of $\hat{\theta}$ can be found in other ways (e.g. continuous mapping theorem, WLLN).
\end{remark}
 
The following theorem gives a sufficient conditions for a sequence of MLEs $\hat{\theta}_n$ based on a sample $X_1, \dots, X_n \SimIID f_\theta$ to be asymptotically normal. Let $\theta_0 \in \Theta$ be the true parameter.
\begin{thm}
	If the following conditions hold
	\begin{enumerate}
		\item[(A1)] The score function $\psi$ is well-defined and $0< I(\theta) < \infty$;
		\item[(A2)] $\frac{\partial^2}{\partial \theta^2} \psi(x; \theta)$ is continuous;
		\item[(A3)] For some $\epsilon$, $g$ such that $\EE_{\theta_0} g(X) < \infty$,
		$$ \sup_{| \theta - \theta_0 |\leq \epsilon} |\frac{\partial^2}{\partial \theta^2} \psi(x; \theta)| < g(x);$$
	\end{enumerate}
	and $\hat{\theta}_n$ exists, is unique, and is consistent under $H_0$, then 
	\begin{align*}
		\hat{\theta} = \theta_0 + \frac{1}{n I(\theta_0)} \sum_{i=1}^n \psi(X_i; \theta) + o_{p}(n^{-1/2}),
	\end{align*} 
	and 
	{\color{blue}
	\begin{align*}
		\sqrt{n} (\hat{\theta} - \theta_0) \xrightarrow[\theta_0]{D} N(0, I^{-1}(\theta_0) ).
	\end{align*}}
\end{thm}
\begin{remark}
	For suitable $h$, we can also show AN of $h(\hat{\theta})$ using the delta-method.
\end{remark}

\begin{exap}
	$X_i \SimIID \mathrm{Gamma}(\alpha, 1)$. The MLE $\hat{\alpha}$ is the solution to
	$$\frac{n \Gamma'(\alpha)}{\Gamma(\alpha)} = \sum_{i=1}^n \log(X_i).$$
	
	It can only be computed numerically. If we want to do inference for $\alpha$, since 
	$$I(\alpha) = - \EE_\alpha ( \frac{\partial^2}{\partial \alpha^2} \log f_\alpha(x)) = \frac{ \Gamma''(\alpha)\Gamma(\alpha) - \Gamma'(\alpha)^2 }{\Gamma(\alpha)^2},$$
	$$\sqrt{nI(\alpha)}(\hat{\alpha} - \alpha) \xrightarrow[\alpha]{D} N(0,1).$$
\end{exap}

\begin{exap}
	$X_i \SimIID U(0, \theta)$. The conditons for AN do not hold. Its MLE is $\hat{\theta} = X_{(n)}$. So
	$$n(\theta - \hat{\theta}) \xrightarrow{D} \mathrm{Exp}(\theta).$$
\end{exap}