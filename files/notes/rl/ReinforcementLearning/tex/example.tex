\section*{Motivation}
Let $F(x,y) = ax^2 + by^2$. And $f_1(x,y) = ax^2$, $f_2(x,y) = by^2$. Consider the following problem:
$$\min_{x,y} \quad \sum_{i=1}^2 f(x,y).$$
Let $W = (x, y) \in \RR^{ 1\times 2 }$. 
$$f_1(W) =  \begin{pmatrix}
\sqrt{a} & 0
\end{pmatrix} W^T W \begin{pmatrix}
\sqrt{a} \\ 
0
\end{pmatrix}$$
$$f_2(W) =  \begin{pmatrix}
0 & \sqrt{b}
\end{pmatrix} W^T W \begin{pmatrix}
0 \\ 
\sqrt{b}
\end{pmatrix}$$
For convenience, let $x_1 = (\sqrt{a}, 0)^T$ and $x_2 = (0, \sqrt{b} )^T$. The gradient of $F(W)$ is 
$$\nabla F(W) = 2W ( x_1 x_1^T + x_2 x_2^T ).$$
Let $\xi_k \sin \mathrm{Bin}(1, \frac{1}{2})$. The $(k+1)$-th SGD iterate is 
$$W_{k+1} = W_k ({ I_2 - 2\alpha   x_{\xi_k} x_{\xi_k}^T} ).$$ 
Define $M = ({ I_2 - 2\alpha   x_{1} x_{1}^T} )({ I_2 - 2\alpha   x_{2} x_{2}^T} )$. Obviously, $\|M\|_2 < 1$ if $\alpha < \min\{ \frac{1}{2a}, \frac{1}{2b} \}$. Then 
$$\| W_n \|_2 \leq \| M \|_2^{N(n)}   $$
where $N(n)$ represent there are $N(n)$ $M$ appear in $W_n$ (of course, $N(n) \to \infty$). 
\section*{Ideas}
We want to use SGD to solve the following convex optimization problem:
$$\min_{W} \quad F(W) = \sum_{i=1}^n f_i(W).$$ \

~\\

\noindent
\textbf{Assumption 1}: Assume The gradient of $f_i$ is 
\begin{align*}
	\nabla f_i(W) = (W - W^\ast)\cdot G(W, i).  
\end{align*} 
It is the Taylor expansion of $\nabla f_i$ around the stationary point of $F$. Then let $\xi_k$ be discrete uniform random variable on $\{1,\dots, n\}$. The SGD iterate is
$$W_{k+1} \leftarrow W_k - \alpha \nabla f_{\xi_i}(W_k) = ( W_k - W^\ast) ( I -   \alpha G(W_k, \xi_{k+1}) ) + W^\ast.$$
Let $X_j = I - \alpha G(W_j, \xi_j)$ be a matrix-value Markovian process; and $\Pi^k_1 = \prod_{j=1}^k X_j$. Then the $(k+1)$-th step SGD iterate is 
$$W_{k+1}- W^\ast = ( W_K-W^\ast) \cdot \Pi^{k+1}_{K+1} .$$ 


~\\

\noindent
\textbf{Assumption 2}: Assume the following condition holds
$$\sup_W \| G(W, j) \|_2 < \infty $$
for all $j = 1, \dots, n$. Under this assumption, we can choose an appropriate learning rate, for example, 
$$\alpha < 1/ \max_j( \sup_W \| G(W, j) \|_2)$$
such that there exits at least one eigenvalue of $X_j$ strictly less than $1$ for all $j$.  

~\\

\noindent
\textbf{Note}: Remains to prove $W_k$ converges to the optimal solution.

\begin{proof}
	Follow the classical setting: by Lemma 4.2 and the strongly convex assumption, 
	\begin{align*}
		\EE_{\xi_k} F(W_{k+1}) - F(W_k) &\leq  -2\alpha c ( F(W_k) - F_\ast ) + \frac{1}{2}\alpha^2 L \EE_{\xi_k}[ \|  (W_k - W^\ast) \cdot G(W_k, \xi_{k+1}) \|^2_{F} ] \\
		&\leq  -2\alpha c ( F(W_k) - F_\ast ) + \frac{d}{2}\alpha^2 L   \|  W_k - W^\ast \|_F  \cdot  \EE_{\xi_k}  \| G(W_k, \xi_{k+1}) \|^2_{2}   
	\end{align*} 
 	Re-arrange it and take total expectation:
 	\begin{align*}
 	\EE  F(W_{k+1}) - F_\ast &\leq  (1-2\alpha c) ( F(W_k) - F_\ast ) + \frac{d}{2}\alpha^2 L \EE  \|  W_k - W^\ast\|_F \cdot \EE \| G(W_k, \xi_{k+1}) \|^2_{F}  
 	\end{align*} 
 	\textbf{Need to know when $\Pi^k \to 0$.}
\end{proof} 
 
\newpage

\section*{Toy Example}
Let $x_i, y_i \in \RR^d$ for $i = 1, \dots, n$. Consider the following optimization problem:
\begin{align*}
	\min_{W \in \RR^{r\times d}} \quad F(W) = \sum_{i=1}^n (x_i^T W^T W x_i + \IndOne^T W y_i)
\end{align*}  
where $r \leq d$.
\begin{enumerate}[a)]
	\item Prove this is a convex optimization problem.
	\begin{proof}
		Compute $\nabla F$:
		\begin{align*}
			\nabla F &= \frac{\partial F}{\partial W} \\
			&= 2 W \sum_{i=1}^n x_i x_i^T + \sum_{i=1}^n \IndOne y_i^T  \tag{1}
		\end{align*}
		
		Compute the Hessian matrix:
		\begin{align*}
			\mathbf{H} &= \frac{\partial^2 F}{\partial W^2} \\
			&=  2 \sum_{i=1}^n x_i x_i^T
		\end{align*}
		
		It is easy to see that all eigenvalues of $\mathbf{H}$ is non-negative. Therefore, $\mathbf{H}$ is positive semi-definite. It implies that $F(W)$ is convex. 
	\end{proof}

	\item Assume the initial point is $W_0$. Compute the $k$-th step iterate of SGD with the constant learning rate $\alpha$, $W_k$.
	\begin{proof}
		Let $\{\xi_j\}_{j=1,\dots, k}$ be iid discrete uniform random variable on $\{1, 2, \dots, n\}$. The first step SGD iterate $W_1$ is 
		$$W_1 = W_0 - \alpha \cdot 2W_0 x_{\xi_1} x_{\xi_1}^T - \alpha  \IndOne y_{\xi_i}^T= W_0 ( I_d - 2\alpha x_{\xi_1} x_{\xi_1}^T ) - \alpha \IndOne y_{\xi_i}^T.$$
		Denote it by
		$$W_{k+1} = W_k G_{k+1} - H_{k+1}.$$
		By induction, it is obvious that 
		$$W_{k+1} = W_0 \prod_{i=1}^{k+1}G_{k} - \left( H_{k+1} + H_kG_{k+1} + \dots + H_1G_2 G_3 \cdots G_{k+1}  \right) $$
	\end{proof}

	\item Assume $2 \alpha \cdot{\sup_i \|x_i \|_2^2 } < 1$ and $d=2$. Prove the largest eigenvalue of $W_k$ converge to $0$ in probability.
	
	\begin{proof}
	 	We need the following lemma:
	 	\begin{lemma}
	 		Let $x_1, x_2 \in \RR^d$ ($d = 2$) be linearly independent with length strictly less than $1$. Define $$M = (I_d - x_1 x_1^T)(I_d - x_2 x_2^T).$$ If $x_1$ and $x_2$ are linearly independent, then
	 		$$\|M\|_2<1.$$
	 	\end{lemma}
 		WLOG, we can consider the following scheme: Let $\{ x_i \}_{i=1,2} \in \RR^2$ be linearly independent with $x_i^T x_i < 1$. Define $\{\xi_j\}_{j=1,2,\dots}$ as iid discrete uniform random variables on $\{1,2 \}$. Then
 		$$X_j =  I_2 - x_{\xi_j} x_{\xi_j}^T $$
 		is a sequence iid random matrices. Let $\Pi_n = \prod_{i=1}^n X_i$. It suffices to prove $\| \Pi_n \|_2 \to 0$ in probability.
 		
 		Assume $M$ appears $N(n)$ times before $n$. By sub-multiplicativity,
 		$$\| \Pi_n \|_2 \leq \| M \|_2^N \prod_j \|  X_j \|_2 \leq \| M \|_2^N \to 0$$
 		as $n\to \infty$. (It remains to prove $N(n) \to \infty$ as $n\to \infty$; but it is obvious.)
	\end{proof}
\end{enumerate}
 
 
\section*{Complement}
\begin{example}
	Let $x_1, x_2 \in \RR^d$ ($d = 2$) with length strictly less than $1$. Define $$M = (I_d - x_1 x_1^T)(I_d - x_2 x_2^T).$$ If $x_1$ and $x_2$ are linearly independent, then the largest eigenvalue of $M$ is strictly less than $1$.
	
	\begin{proof}
		\begin{itemize}
			\item By min-max principal, the largest eigenvalue of $M$ cannot be larger than $1$. (See \href{https://mathoverflow.net/questions/106191/eigenvalues-of-the-product-of-two-symmetric-matrices/106199#106199}{here}). So it suffices to prove the eigenvalue cannot be equal to $1$.
			
			\item Assume $v = x_1 + b x_2$ is an eigenvector corresponding to the eigenvalue $1$. Compute $Mv$. And compare the coefficient of $x_1$ and of $x_2$.
			
			\item Finally, we notice that we must have $\langle x_1, x_2 \rangle^2 = \| x_1\|_2^2 \|x_2 \|_2^2$. It holds if and only if $x_1$ and $x_2$ are linearly dependent. 
			
			\item This result also holds for $d>2$ (by following the same scheme); but I cannot decide for which eigenvalues it holds.
		\end{itemize}
	\end{proof}
\end{example}

\begin{example}
	Let $\{ x_i \}_{i=1,\dots, d} \in \RR^d$ be linearly independent with $x_i^T x_i < 1$. Define $\{\xi_j\}_{j=1,2,\dots}$ as iid discrete uniform random variables on $\{1,\dots, d \}$. Then
	$$X_j =  I_d - x_{\xi_j} x_{\xi_j}^T $$
	is a sequence iid random matrices. Let $\Pi_n = \prod_{i=1}^n X_i$. Prove $\| \Pi_n \|_2 \to 0$ in probability.
\end{example}

\begin{lemma}
	$$\max \{  \|A\|_2, \|B\|_2  \}\leq \|A + B\|_2 \leq \|A\|_2 + \|B\|_2$$
\end{lemma}